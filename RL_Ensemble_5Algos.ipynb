{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Ensemble_5Algos.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMvILlruV3V1KaRgZElqOme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poohi5/StockTradingusingRL/blob/main/RL_Ensemble_5Algos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERYHDtUjHic1"
      },
      "source": [
        "# CMPE - 297 - SEC 47 \n",
        "## Reinforcement Learning Project\n",
        "## Team Members - Akshaya Nagarajan, Pooja Patil "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqdEb7WbH2Xy"
      },
      "source": [
        "# Automated Stock Trading using Deep Reinforcement Learning\n",
        "## In this project we implement an Ensemble Strategy using Reinforcement Learning to trade stocks through Open AI Gym environment with an end goal of maximizing total returns. \n",
        "\n",
        "----\n",
        "**Reference**\n",
        "Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy. In ICAIF ’20: ACM International Conference on AI in Finance, Oct. 15–16, 2020, Manhattan, NY. ACM, New York, NY, USA.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVOOir73JJlc",
        "outputId": "afd4769b-d93c-4bf9-8a9a-51bd132517f2"
      },
      "source": [
        "!pip install stockstats\n",
        "!pip install git+https://github.com/quantopian/pyfolio\n",
        "!pip install yfinance\n",
        "!pip install gym\n",
        "!pip install stable-baselines[mpi]\n",
        "!pip install tensorflow==1.15.4    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stockstats\n",
            "  Downloading https://files.pythonhosted.org/packages/32/41/d3828c5bc0a262cb3112a4024108a3b019c183fa3b3078bff34bf25abf91/stockstats-0.3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from stockstats) (1.18.5)\n",
            "Collecting int-date>=0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/43/27/31803df15173ab341fe7548c14154b54227dfd8f630daa09a1c6e7db52f7/int_date-0.1.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from stockstats) (1.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.6/dist-packages (from int-date>=0.1.7->stockstats) (2.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from int-date>=0.1.7->stockstats) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.1->stockstats) (2018.9)\n",
            "Installing collected packages: int-date, stockstats\n",
            "Successfully installed int-date-0.1.8 stockstats-0.3.2\n",
            "Collecting git+https://github.com/quantopian/pyfolio\n",
            "  Cloning https://github.com/quantopian/pyfolio to /tmp/pip-req-build-dr0l5res\n",
            "  Running command git clone -q https://github.com/quantopian/pyfolio /tmp/pip-req-build-dr0l5res\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (5.5.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (1.1.4)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (0.22.2.post1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (0.11.0)\n",
            "Collecting empyrical>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/43/1b997c21411c6ab7c96dc034e160198272c7a785aeea7654c9bcf98bec83/empyrical-0.5.5.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (4.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (50.3.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->pyfolio==0.9.2+75.g4b901f6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->pyfolio==0.9.2+75.g4b901f6) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->pyfolio==0.9.2+75.g4b901f6) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->pyfolio==0.9.2+75.g4b901f6) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16.1->pyfolio==0.9.2+75.g4b901f6) (0.17.0)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.6/dist-packages (from empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.6.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (1.24.3)\n",
            "Building wheels for collected packages: pyfolio, empyrical\n",
            "  Building wheel for pyfolio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfolio: filename=pyfolio-0.9.2+75.g4b901f6-cp36-none-any.whl size=75764 sha256=5d4c9e1906a7dd0ae5d60d0bb60a2e5cf75ed6d049362f252b8fb3a8ee620096\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uowhhaxd/wheels/62/7d/a7/3e462442ba7d63c35414176627c886340521dc3dbc0893ce9f\n",
            "  Building wheel for empyrical (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empyrical: filename=empyrical-0.5.5-cp36-none-any.whl size=39763 sha256=d8f1df79aba45ef3cb3a2c113028b5caa683e1072236537cddd3b4e33930e53a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/b2/c8/6769d8444d2f2e608fae2641833110668d0ffd1abeb2e9f3fc\n",
            "Successfully built pyfolio empyrical\n",
            "Installing collected packages: empyrical, pyfolio\n",
            "Successfully installed empyrical-0.5.5 pyfolio-0.9.2+75.g4b901f6\n",
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.4)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/78/56a7c88a57d0d14945472535d0df9fb4bbad7d34ede658ec7961635c790e/lxml-4.6.2-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22618 sha256=06255b1cb096f7be59863df2bbdc73a1f814f3fd2337f2793aebc3f87d02836c\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.2 yfinance-0.1.55\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Collecting stable-baselines[mpi]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl (240kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.1.4)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Collecting mpi4py; extra == \"mpi\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/8f/bbd8de5ba566dd77e408d8136e2bab7fdf2b97ce06cab830ba8b50a2f588/mpi4py-3.0.3.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->stable-baselines[mpi]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.0.3-cp36-cp36m-linux_x86_64.whl size=2074450 sha256=8d0da38f0358fa04db2f9e1851b8bf16efe3832306dcf90d4c1f770dadea138d\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/e0/86/2b713dd512199096012ceca61429e12b960888de59818871d6\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py, stable-baselines\n",
            "Successfully installed mpi4py-3.0.3 stable-baselines-2.10.1\n",
            "Collecting tensorflow==1.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/64/7a19837dd54d3f53b1ce5ae346ab401dde9678e8f233220317000bfdb3e2/tensorflow-1.15.4-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.35.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.10.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.8.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 49.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.33.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (3.12.4)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.4) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=e85f0459607126f6ce2f6a2b372b78426932006fb4630d59fd477edb7e79b82d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHe7r56QxTMe",
        "outputId": "d7d5612a-9ef6-4110-b413-57a77ac387e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/RLProject\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71RtH3B_KLm5"
      },
      "source": [
        "## Data Collection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzgBfbMOKDMl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from gym.utils import seeding\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from stockstats import StockDataFrame as Sdf\n",
        "import yfinance as yf\n",
        "import pyfolio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bka7mBSkLKoR"
      },
      "source": [
        "dow_30_ticker = ['AAPL','MSFT','JPM','V','RTX','PG','GS','NKE','DIS','AXP',\n",
        "                  'HD','INTC','WMT','IBM','MRK','UNH','KO','CAT','TRV','JNJ',\n",
        "                  'CVX','MCD','VZ','CSCO','XOM','BA','MMM','PFE','WBA','DD']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzWp7KI5fZdf"
      },
      "source": [
        "##### **AXP** - American Express\n",
        "##### **PRCCD** — Price – Close – Daily\n",
        "##### **AJEXDI** = Cumulative Adjustment Factor (Issue) Ex-Date.\n",
        "##### **prchd** -  The high price (PRCHD) is the highest trade price for the date. \n",
        "##### **prcld** -  The low price (PRCLD) is the lowest trade price for the date.\n",
        "##### **CSHTRD** - Daily trading volume of a stock\n",
        "##### **adjcp** - Adjusted close price\n",
        "##### **MACD** - Moving Average Convergence Divergence\n",
        "##### **RSI**: Relative Strength Index (RSI) is calculated using close price.\n",
        "##### **CCI**: Commodity Channel Index (CCI) is calculated using high, low and close price.\n",
        "##### **ADX**: Average Directional Index (ADX) is calculated using high, low and close price.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx7pEl7nLNDx"
      },
      "source": [
        "def load_dataset_dow30() -> pd.DataFrame:\n",
        "  dow_30 = pd.DataFrame()\n",
        "  for tic in dow_30_ticker:\n",
        "      data_df = yf.download(tic, start=\"2009-01-01\", end=\"2020-10-23\")\n",
        "      data_df['tic'] = tic\n",
        "      dow_30=dow_30.append(data_df)\n",
        "\n",
        "  dow_30=dow_30.reset_index()\n",
        "  print(dow_30.columns)\n",
        "  dow_30.rename(columns={\"Date\": \"datadate\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Adj Close\": \"adjcp\", \"Volume\": \"volume\", \"tic\": \"tic\"}, inplace = True)\n",
        "  print(dow_30.columns)\n",
        "  return dow_30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqg8V2VcLRyA"
      },
      "source": [
        "def load_dataset(file_name: str) -> pd.DataFrame:\n",
        "    _data = pd.read_csv(file_name)\n",
        "    return _data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnTNYoJdLaok"
      },
      "source": [
        "def data_split(df,start,end):\n",
        "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
        "    data=data.sort_values(['datadate','tic'],ignore_index=True)\n",
        "    data.index = data.datadate.factorize()[0]\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh9ngzr-Lbxr"
      },
      "source": [
        "def calcualte_price(df):\n",
        "    data = df.copy()\n",
        "    data = data[['datadate', 'tic', 'prccd', 'ajexdi', 'prcod', 'prchd', 'prcld', 'cshtrd']]\n",
        "    data['ajexdi'] = data['ajexdi'].apply(lambda x: 1 if x == 0 else x)\n",
        "    data['adjcp'] = data['prccd'] / data['ajexdi']\n",
        "    data['open'] = data['prcod'] / data['ajexdi']\n",
        "    data['high'] = data['prchd'] / data['ajexdi']\n",
        "    data['low'] = data['prcld'] / data['ajexdi']\n",
        "    data['volume'] = data['cshtrd']\n",
        "    data = data[['datadate', 'tic', 'adjcp', 'open', 'high', 'low', 'volume']]\n",
        "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXJSe-XDLc_E"
      },
      "source": [
        "def calcualte_price_dow30(df):\n",
        "    data = df.copy()\n",
        "    data = data[['datadate', 'tic', 'adjcp', 'open', 'high', 'low', 'volume']]\n",
        "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdOhhhB8LeSa"
      },
      "source": [
        "def add_technical_indicator(df):\n",
        "    stock = Sdf.retype(df.copy())\n",
        "    stock['close'] = stock['adjcp']\n",
        "    unique_ticker = stock.tic.unique()\n",
        "    macd = pd.DataFrame()\n",
        "    rsi = pd.DataFrame()\n",
        "    cci = pd.DataFrame()\n",
        "    dx = pd.DataFrame()\n",
        "\n",
        "    for i in range(len(unique_ticker)):\n",
        "        ## macd\n",
        "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
        "        temp_macd = pd.DataFrame(temp_macd)\n",
        "        macd = macd.append(temp_macd, ignore_index=True)\n",
        "        ## rsi\n",
        "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
        "        temp_rsi = pd.DataFrame(temp_rsi)\n",
        "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
        "        ## cci\n",
        "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
        "        temp_cci = pd.DataFrame(temp_cci)\n",
        "        cci = cci.append(temp_cci, ignore_index=True)\n",
        "        ## adx\n",
        "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
        "        temp_dx = pd.DataFrame(temp_dx)\n",
        "        dx = dx.append(temp_dx, ignore_index=True)\n",
        "\n",
        "\n",
        "    df['macd'] = macd\n",
        "    df['rsi'] = rsi\n",
        "    df['cci'] = cci\n",
        "    df['adx'] = dx\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tlnJYToLfa8"
      },
      "source": [
        "def preprocess_data():\n",
        "    df = load_dataset(TRAINING_DATA_FILE)\n",
        "    #df = load_dataset_dow30()\n",
        "\n",
        "    print(df.head(2))\n",
        "\n",
        "    # Null checks\n",
        "    print(df.isnull().values.any())\n",
        "    df = df.dropna()\n",
        "    print(df.tic.value_counts())\n",
        "    print(df.isnull().values.any())\n",
        "    \n",
        "    # get data after 2009\n",
        "    df = df[df.datadate>=20090000]\n",
        "    #df = df[df.datadate>=\"2009-01-01\"]\n",
        "    \n",
        "    # calcualte adjusted price\n",
        "    df_preprocess = calcualte_price(df)\n",
        "    #df_preprocess = calcualte_price_dow30(df)\n",
        "    # add technical indicators using stockstats\n",
        "\n",
        "    df_final=add_technical_indicator(df_preprocess)\n",
        "    print(df_final.isnull().values.any())\n",
        "    print(df_final.isna().any())\n",
        "    print(df_final.tail(3))\n",
        "    \n",
        "    # fill the missing values at the beginning\n",
        "    df_final.fillna(method='bfill',inplace=True)\n",
        "    print(df_final.tail(3))\n",
        "    \n",
        "    return df_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmHNnmLVLhDM"
      },
      "source": [
        "def add_turbulence(df):\n",
        "    turbulence_index = calcualte_turbulence(df)\n",
        "    df = df.merge(turbulence_index, on='datadate')\n",
        "    df = df.sort_values(['datadate','tic']).reset_index(drop=True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LW0xro6Liqh"
      },
      "source": [
        "def calcualte_turbulence(df):\n",
        "    df_price_pivot=df.pivot(index='datadate', columns='tic', values='adjcp')\n",
        "    unique_date = df.datadate.unique()\n",
        "    # start after a year\n",
        "    start = 252\n",
        "    turbulence_index = [0]*start\n",
        "    #turbulence_index = [0]\n",
        "    count=0\n",
        "    for i in range(start,len(unique_date)):\n",
        "        current_price = df_price_pivot[df_price_pivot.index == unique_date[i]]\n",
        "        hist_price = df_price_pivot[[n in unique_date[0:i] for n in df_price_pivot.index ]]\n",
        "        cov_temp = hist_price.cov()\n",
        "        current_temp=(current_price - np.mean(hist_price,axis=0))\n",
        "        temp = current_temp.values.dot(np.linalg.inv(cov_temp)).dot(current_temp.values.T)\n",
        "        if temp>0:\n",
        "            count+=1\n",
        "            if count>2:\n",
        "                turbulence_temp = temp[0][0]\n",
        "            else:\n",
        "                #avoid large outlier because of the calculation just begins\n",
        "                turbulence_temp=0\n",
        "        else:\n",
        "            turbulence_temp=0\n",
        "        turbulence_index.append(turbulence_temp)\n",
        "    \n",
        "    \n",
        "    turbulence_index = pd.DataFrame({'datadate':df_price_pivot.index, 'turbulence':turbulence_index})\n",
        "    return turbulence_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtwKBZL4LkNb"
      },
      "source": [
        "TRAINING_DATA_FILE = root_dir + '/data/dow_30_2009_2020.csv'\n",
        "TURBULENCE_DATA = root_dir + '/data/dow30_turbulence_index.csv'\n",
        "TRAINED_MODEL_DIR = root_dir + '/trained_models'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2MTcTzeLsEB",
        "outputId": "1017c0a4-a4d2-4487-e084-58c10fe5cdf2"
      },
      "source": [
        "from stable_baselines import PPO2, DDPG, A2C, ACKTR, TD3, TRPO, GAIL, ACER\n",
        "from stable_baselines import DDPG\n",
        "from stable_baselines import A2C\n",
        "from stable_baselines import SAC\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy\n",
        "from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhoi1JBwMPG5"
      },
      "source": [
        "### Setting variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC3SLV9XMOze"
      },
      "source": [
        "# 100 shares per trade\n",
        "MAX_NORMALIZE = 100\n",
        "# initial amount of money we have in account\n",
        "INITIAL_ACC_BAL=1000000\n",
        "# total number of stocks in  portfolio\n",
        "TOTAL_STOCKS = 30\n",
        "# transaction fee: 1/1000 reasonable percentage\n",
        "TRANSACTION_FEE_PERCENT = 0.001\n",
        "REWARD_SCALING = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCzFniYQMYmC"
      },
      "source": [
        "## Training Gym environment\n",
        "\n",
        "1.   Define action_space and obeservation_space in the Environment's Constructor\n",
        "2.   Reset Method - reset environment \n",
        "3.   Define Sell Stock Method\n",
        "4.   Define Buy Stock Method\n",
        "5.   Define Step function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAejo8PtMHjE"
      },
      "source": [
        "#Reference https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\n",
        "# https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020\n",
        "\n",
        "class StockTrainEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df,day = 0):\n",
        "        self.day = day\n",
        "        self.df = df\n",
        "\n",
        "        # action_space \n",
        "        self.action_space = spaces.Box(low = -1, high = 1,shape = (TOTAL_STOCKS,)) \n",
        "        \n",
        "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
        "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
        "        \n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.terminal = False             \n",
        "        \n",
        "        # initalize state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist() + \\\n",
        "                      self.data.cci.values.tolist() + \\\n",
        "                      self.data.adx.values.tolist()\n",
        "        # initialize reward\n",
        "        self.reward = 0\n",
        "        self.cost = 0\n",
        "        # memorize all the total balance change\n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.rewards_memory = []\n",
        "        self.trades = 0\n",
        "        #self.reset()\n",
        "        self._seed()\n",
        "\n",
        "\n",
        "    def _sell_stock(self, index, action):\n",
        "        # perform sell action based on the sign of the action\n",
        "        if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "            #update balance\n",
        "            self.state[0] += \\\n",
        "            self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "             (1- TRANSACTION_FEE_PERCENT)\n",
        "\n",
        "            self.state[index+TOTAL_STOCKS+1] -= min(abs(action), self.state[index+TOTAL_STOCKS+1])\n",
        "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "             TRANSACTION_FEE_PERCENT\n",
        "            self.trades+=1\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    \n",
        "    def _buy_stock(self, index, action):\n",
        "        # perform buy action based on the sign of the action\n",
        "        available_amount = self.state[0] // self.state[index+1]\n",
        "        # print('available_amount:{}'.format(available_amount))\n",
        "\n",
        "        #update balance\n",
        "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
        "                          (1+ TRANSACTION_FEE_PERCENT)\n",
        "\n",
        "        self.state[index+TOTAL_STOCKS+1] += min(available_amount, action)\n",
        "\n",
        "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
        "                          TRANSACTION_FEE_PERCENT\n",
        "        self.trades+=1\n",
        "        \n",
        "    def step(self, actions):\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "\n",
        "        if self.terminal:\n",
        "            plt.plot(self.asset_memory,'r')\n",
        "            plt.savefig('account_value_train.png')\n",
        "            plt.close()\n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            \n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            df_total_value = pd.DataFrame(self.asset_memory)\n",
        "            df_total_value.to_csv('account_value_train.csv')\n",
        "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
        "            #print(\"total_cost: \", self.cost)\n",
        "            #print(\"total_trades: \", self.trades)\n",
        "            df_total_value.columns = ['account_value']\n",
        "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
        "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
        "                  df_total_value['daily_return'].std()\n",
        "            #print(\"Sharpe: \",sharpe)\n",
        "            #print(\"=================================\")\n",
        "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
        "            #df_rewards.to_csv('account_rewards_train.csv')\n",
        "            \n",
        "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
        "            #with open('obs.pkl', 'wb') as f:  \n",
        "            #    pickle.dump(self.state, f)\n",
        "            \n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            # print(np.array(self.state[1:29]))\n",
        "\n",
        "            actions = actions * MAX_NORMALIZE\n",
        "            #actions = (actions.astype(int))\n",
        "            \n",
        "            begin_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
        "            \n",
        "            argsort_actions = np.argsort(actions)\n",
        "            \n",
        "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
        "\n",
        "            for index in sell_index:\n",
        "                # print('take sell action'.format(actions[index]))\n",
        "                self._sell_stock(index, actions[index])\n",
        "\n",
        "            for index in buy_index:\n",
        "                # print('take buy action: {}'.format(actions[index]))\n",
        "                self._buy_stock(index, actions[index])\n",
        "\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]         \n",
        "            #load next state\n",
        "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
        "            self.state =  [self.state[0]] + \\\n",
        "                    self.data.adjcp.values.tolist() + \\\n",
        "                    list(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]) + \\\n",
        "                    self.data.macd.values.tolist() + \\\n",
        "                    self.data.rsi.values.tolist() + \\\n",
        "                    self.data.cci.values.tolist() + \\\n",
        "                    self.data.adx.values.tolist()\n",
        "            \n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            self.asset_memory.append(end_total_asset)\n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            \n",
        "            self.reward = end_total_asset - begin_total_asset            \n",
        "            # print(\"step_reward:{}\".format(self.reward))\n",
        "            self.rewards_memory.append(self.reward)\n",
        "            \n",
        "            self.reward = self.reward*REWARD_SCALING\n",
        "\n",
        "\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        self.terminal = False \n",
        "        self.rewards_memory = []\n",
        "        #initiate state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist() + \\\n",
        "                      self.data.cci.values.tolist() + \\\n",
        "                      self.data.adx.values.tolist() \n",
        "        # iteration += 1 \n",
        "        return self.state\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        return self.state\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZwpa6sBOZdS"
      },
      "source": [
        "# Validation Gym Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK7UDkBLOFMS"
      },
      "source": [
        "class StockValidationEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df, day = 0, turbulence_threshold=140, iteration=''):\n",
        "        self.day = day\n",
        "        self.df = df\n",
        "        \n",
        "        # action_space normalization and shape is TOTAL_STOCKS\n",
        "        self.action_space = spaces.Box(low = -1, high = 1,shape = (TOTAL_STOCKS,)) \n",
        "        \n",
        "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
        "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.terminal = False     \n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        \n",
        "        # initalize state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist() + \\\n",
        "                      self.data.cci.values.tolist() + \\\n",
        "                      self.data.adx.values.tolist()\n",
        "        # initialize reward\n",
        "        self.reward = 0\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        # memorize all the total balance change\n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.rewards_memory = []\n",
        "        #self.reset()\n",
        "        self._seed()\n",
        "        \n",
        "        self.iteration=iteration\n",
        "\n",
        "\n",
        "    def _sell_stock(self, index, action):\n",
        "        # perform sell action based on the sign of the action\n",
        "        if self.turbulence<self.turbulence_threshold:\n",
        "            if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "                #update balance\n",
        "                self.state[0] += \\\n",
        "                self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "                 (1- TRANSACTION_FEE_PERCENT)\n",
        "                \n",
        "                self.state[index+TOTAL_STOCKS+1] -= min(abs(action), self.state[index+TOTAL_STOCKS+1])\n",
        "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "                 TRANSACTION_FEE_PERCENT\n",
        "                self.trades+=1\n",
        "            else:\n",
        "                pass\n",
        "        else:\n",
        "            # if turbulence goes over threshold, just clear out all positions \n",
        "            if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "                #update balance\n",
        "                self.state[0] += self.state[index+1]*self.state[index+TOTAL_STOCKS+1]* \\\n",
        "                              (1- TRANSACTION_FEE_PERCENT)\n",
        "                self.state[index+TOTAL_STOCKS+1] =0\n",
        "                self.cost += self.state[index+1]*self.state[index+TOTAL_STOCKS+1]* \\\n",
        "                              TRANSACTION_FEE_PERCENT\n",
        "                self.trades+=1\n",
        "            else:\n",
        "                pass\n",
        "    \n",
        "    def _buy_stock(self, index, action):\n",
        "        # perform buy action based on the sign of the action\n",
        "        if self.turbulence< self.turbulence_threshold:\n",
        "            available_amount = self.state[0] // self.state[index+1]\n",
        "            # print('available_amount:{}'.format(available_amount))\n",
        "            \n",
        "            #update balance\n",
        "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
        "                              (1+ TRANSACTION_FEE_PERCENT)\n",
        "\n",
        "            self.state[index+TOTAL_STOCKS+1] += min(available_amount, action)\n",
        "            \n",
        "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
        "                              TRANSACTION_FEE_PERCENT\n",
        "            self.trades+=1\n",
        "        else:\n",
        "            # if turbulence goes over threshold, just stop buying\n",
        "            pass\n",
        "        \n",
        "    def step(self, actions):\n",
        "        # print(self.day)\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "        # print(actions)\n",
        "\n",
        "        if self.terminal:\n",
        "            plt.plot(self.asset_memory,'r')\n",
        "            plt.savefig('account_value_validation_{}.png'.format(self.iteration))\n",
        "            plt.close()\n",
        "            df_total_value = pd.DataFrame(self.asset_memory)\n",
        "            df_total_value.to_csv('account_value_validation_{}.csv'.format(self.iteration))\n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            #print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
        "\n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):61]))- self.asset_memory[0] ))\n",
        "            #print(\"total_cost: \", self.cost)\n",
        "            #print(\"total trades: \", self.trades)\n",
        "\n",
        "            df_total_value.columns = ['account_value']\n",
        "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
        "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
        "                  df_total_value['daily_return'].std()\n",
        "            #print(\"Sharpe: \",sharpe)\n",
        "            \n",
        "            #df_rewards = pd.DataFrame(self.rewards_memory)\n",
        "            #df_rewards.to_csv('account_rewards_trade_{}.csv'.format(self.iteration))\n",
        "            \n",
        "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
        "            #with open('obs.pkl', 'wb') as f:  \n",
        "            #    pickle.dump(self.state, f)\n",
        "            \n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            # print(np.array(self.state[1:29]))\n",
        "\n",
        "            actions = actions * MAX_NORMALIZE\n",
        "            #actions = (actions.astype(int))\n",
        "            if self.turbulence>=self.turbulence_threshold:\n",
        "                actions=np.array([-MAX_NORMALIZE]*TOTAL_STOCKS)\n",
        "            begin_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
        "            \n",
        "            argsort_actions = np.argsort(actions)\n",
        "            \n",
        "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
        "\n",
        "            for index in sell_index:\n",
        "                # print('take sell action'.format(actions[index]))\n",
        "                self._sell_stock(index, actions[index])\n",
        "\n",
        "            for index in buy_index:\n",
        "                # print('take buy action: {}'.format(actions[index]))\n",
        "                self._buy_stock(index, actions[index])\n",
        "\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]         \n",
        "            self.turbulence = self.data['turbulence'].values[0]\n",
        "            #print(self.turbulence)\n",
        "            #load next state\n",
        "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
        "            self.state =  [self.state[0]] + \\\n",
        "                    self.data.adjcp.values.tolist() + \\\n",
        "                    list(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]) + \\\n",
        "                    self.data.macd.values.tolist() + \\\n",
        "                    self.data.rsi.values.tolist() + \\\n",
        "                    self.data.cci.values.tolist() + \\\n",
        "                    self.data.adx.values.tolist()\n",
        "            \n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            self.asset_memory.append(end_total_asset)\n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            \n",
        "            self.reward = end_total_asset - begin_total_asset            \n",
        "            # print(\"step_reward:{}\".format(self.reward))\n",
        "            self.rewards_memory.append(self.reward)\n",
        "            \n",
        "            self.reward = self.reward*REWARD_SCALING\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):  \n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        self.terminal = False \n",
        "        #self.iteration=self.iteration\n",
        "        self.rewards_memory = []\n",
        "        #initiate state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist()  + \\\n",
        "                      self.data.cci.values.tolist()  + \\\n",
        "                      self.data.adx.values.tolist() \n",
        "            \n",
        "        return self.state\n",
        "    \n",
        "    def render(self, mode='human',close=False):\n",
        "        return self.state\n",
        "    \n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9pNOuTCmf-Y"
      },
      "source": [
        "## Trading Gym Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjd84yMHOkqu"
      },
      "source": [
        "class StockTradeEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df,day = 0,turbulence_threshold=140\n",
        "                 ,initial=True, previous_state=[], model_name='', iteration=''):\n",
        "        #money = 10 , scope = 1\n",
        "        self.day = day\n",
        "        self.df = df\n",
        "        self.initial = initial\n",
        "        self.previous_state = previous_state\n",
        "        \n",
        "        # action_space normalization and shape is TOTAL_STOCKS\n",
        "        self.action_space = spaces.Box(low = -1, high = 1,shape = (TOTAL_STOCKS,)) \n",
        "        \n",
        "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
        "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
        "        \n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.terminal = False     \n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        \n",
        "        # initalize state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist() + \\\n",
        "                      self.data.cci.values.tolist() + \\\n",
        "                      self.data.adx.values.tolist()\n",
        "        # initialize reward\n",
        "        self.reward = 0\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        # memorize all the total balance change\n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.rewards_memory = []\n",
        "        #self.reset()\n",
        "        self._seed()\n",
        "        self.model_name=model_name        \n",
        "        self.iteration=iteration\n",
        "\n",
        "\n",
        "    def _sell_stock(self, index, action):\n",
        "        # perform sell action based on the sign of the action\n",
        "        if self.turbulence<self.turbulence_threshold:\n",
        "            if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "                #update balance\n",
        "                self.state[0] += \\\n",
        "                self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "                 (1- TRANSACTION_FEE_PERCENT)\n",
        "                \n",
        "                self.state[index+TOTAL_STOCKS+1] -= min(abs(action), self.state[index+TOTAL_STOCKS+1])\n",
        "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "                 TRANSACTION_FEE_PERCENT\n",
        "                self.trades+=1\n",
        "            else:\n",
        "                pass\n",
        "        else:\n",
        "            # if turbulence goes over threshold, just clear out all positions \n",
        "            if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "                #update balance\n",
        "                self.state[0] += self.state[index+1]*self.state[index+TOTAL_STOCKS+1]* \\\n",
        "                              (1- TRANSACTION_FEE_PERCENT)\n",
        "                self.state[index+TOTAL_STOCKS+1] =0\n",
        "                self.cost += self.state[index+1]*self.state[index+TOTAL_STOCKS+1]* \\\n",
        "                              TRANSACTION_FEE_PERCENT\n",
        "                self.trades+=1\n",
        "            else:\n",
        "                pass\n",
        "    \n",
        "    def _buy_stock(self, index, action):\n",
        "        # perform buy action based on the sign of the action\n",
        "        if self.turbulence< self.turbulence_threshold:\n",
        "            available_amount = self.state[0] // self.state[index+1]\n",
        "            # print('available_amount:{}'.format(available_amount))\n",
        "            \n",
        "            #update balance\n",
        "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
        "                              (1+ TRANSACTION_FEE_PERCENT)\n",
        "\n",
        "            self.state[index+TOTAL_STOCKS+1] += min(available_amount, action)\n",
        "            \n",
        "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
        "                              TRANSACTION_FEE_PERCENT\n",
        "            self.trades+=1\n",
        "        else:\n",
        "            # if turbulence goes over threshold, just stop buying\n",
        "            pass\n",
        "        \n",
        "    def step(self, actions):\n",
        "        # print(self.day)\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "        # print(actions)\n",
        "\n",
        "        if self.terminal:\n",
        "            plt.plot(self.asset_memory,'r')\n",
        "            plt.savefig('account_value_trade_{}_{}.png'.format(self.model_name, self.iteration))\n",
        "            plt.close()\n",
        "            df_total_value = pd.DataFrame(self.asset_memory)\n",
        "            df_total_value.to_csv('account_value_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
        "\n",
        "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))- self.asset_memory[0] ))\n",
        "            print(\"total_cost: \", self.cost)\n",
        "            print(\"total trades: \", self.trades)\n",
        "\n",
        "            df_total_value.columns = ['account_value']\n",
        "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
        "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
        "                  df_total_value['daily_return'].std()\n",
        "            print(\"Sharpe: \",sharpe)\n",
        "            \n",
        "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
        "            df_rewards.to_csv('account_rewards_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
        "            \n",
        "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
        "            #with open('obs.pkl', 'wb') as f:  \n",
        "            #    pickle.dump(self.state, f)\n",
        "            \n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            # print(np.array(self.state[1:29]))\n",
        "\n",
        "            actions = actions * MAX_NORMALIZE\n",
        "            #actions = (actions.astype(int))\n",
        "            if self.turbulence>=self.turbulence_threshold:\n",
        "                actions=np.array([-MAX_NORMALIZE]*TOTAL_STOCKS)\n",
        "                \n",
        "            begin_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
        "            \n",
        "            argsort_actions = np.argsort(actions)\n",
        "            \n",
        "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
        "\n",
        "            for index in sell_index:\n",
        "                # print('take sell action'.format(actions[index]))\n",
        "                self._sell_stock(index, actions[index])\n",
        "\n",
        "            for index in buy_index:\n",
        "                # print('take buy action: {}'.format(actions[index]))\n",
        "                self._buy_stock(index, actions[index])\n",
        "\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]         \n",
        "            self.turbulence = self.data['turbulence'].values[0]\n",
        "            #print(self.turbulence)\n",
        "            #load next state\n",
        "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
        "            self.state =  [self.state[0]] + \\\n",
        "                    self.data.adjcp.values.tolist() + \\\n",
        "                    list(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]) + \\\n",
        "                    self.data.macd.values.tolist() + \\\n",
        "                    self.data.rsi.values.tolist() + \\\n",
        "                    self.data.cci.values.tolist() + \\\n",
        "                    self.data.adx.values.tolist()\n",
        "            \n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            self.asset_memory.append(end_total_asset)\n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            \n",
        "            self.reward = end_total_asset - begin_total_asset            \n",
        "            # print(\"step_reward:{}\".format(self.reward))\n",
        "            self.rewards_memory.append(self.reward)\n",
        "            \n",
        "            self.reward = self.reward*REWARD_SCALING\n",
        "\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):  \n",
        "        if self.initial:\n",
        "            self.asset_memory = [INITIAL_ACC_BAL]\n",
        "            self.day = 0\n",
        "            self.data = self.df.loc[self.day,:]\n",
        "            self.turbulence = 0\n",
        "            self.cost = 0\n",
        "            self.trades = 0\n",
        "            self.terminal = False \n",
        "            #self.iteration=self.iteration\n",
        "            self.rewards_memory = []\n",
        "            #initiate state\n",
        "            self.state = [INITIAL_ACC_BAL] + \\\n",
        "                          self.data.adjcp.values.tolist() + \\\n",
        "                          [0]*TOTAL_STOCKS + \\\n",
        "                          self.data.macd.values.tolist() + \\\n",
        "                          self.data.rsi.values.tolist()  + \\\n",
        "                          self.data.cci.values.tolist()  + \\\n",
        "                          self.data.adx.values.tolist() \n",
        "        else:\n",
        "            previous_total_asset = self.previous_state[0]+ \\\n",
        "            sum(np.array(self.previous_state[1:(TOTAL_STOCKS+1)])*np.array(self.previous_state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            self.asset_memory = [previous_total_asset]\n",
        "            #self.asset_memory = [self.previous_state[0]]\n",
        "            self.day = 0\n",
        "            self.data = self.df.loc[self.day,:]\n",
        "            self.turbulence = 0\n",
        "            self.cost = 0\n",
        "            self.trades = 0\n",
        "            self.terminal = False \n",
        "            #self.iteration=iteration\n",
        "            self.rewards_memory = []\n",
        "            #initiate state\n",
        "            #self.previous_state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]\n",
        "            #[0]*TOTAL_STOCKS + \\\n",
        "\n",
        "            self.state = [ self.previous_state[0]] + \\\n",
        "                          self.data.adjcp.values.tolist() + \\\n",
        "                          self.previous_state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]+ \\\n",
        "                          self.data.macd.values.tolist() + \\\n",
        "                          self.data.rsi.values.tolist()  + \\\n",
        "                          self.data.cci.values.tolist()  + \\\n",
        "                          self.data.adx.values.tolist() \n",
        "            \n",
        "        return self.state\n",
        "    \n",
        "    def render(self, mode='human',close=False):\n",
        "        return self.state\n",
        "    \n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTjYx3bwO6LU"
      },
      "source": [
        "## Models for our Ensemble Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I5Bsiein9q-"
      },
      "source": [
        "1. A2C\n",
        "2. DDPG\n",
        "3. TD3\n",
        "4. PPO\n",
        "5. SAC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqizhF1pO4vT"
      },
      "source": [
        " def train_A2C(env_train, model_name, timesteps=50000):\n",
        "    start = time.time()\n",
        "    model = A2C('MlpPolicy', env_train, verbose=1)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
        "    print('Training time (A2C): ', (end - start) / 60, ' minutes')\n",
        "    return model\n",
        "\n",
        "def train_DDPG(env_train, model_name, timesteps=10000):\n",
        "    # add the noise objects for DDPG\n",
        "    n_actions = env_train.action_space.shape[-1]\n",
        "    param_noise = None\n",
        "    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
        "    start = time.time()\n",
        "    model = DDPG('MlpPolicy', env_train, param_noise=param_noise, action_noise=action_noise)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
        "    print('Training time (DDPG): ', (end-start)/60,' minutes')\n",
        "    return model\n",
        "\n",
        "def train_TD3(env_train, model_name, timesteps=25000):\n",
        "    \"\"\"TD3 model\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    model = TD3('MlpPolicy', env_train, verbose=0)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
        "    print('Training time (TD3): ', (end - start) / 60, ' minutes')\n",
        "    return model\n",
        "\n",
        "def train_PPO(env_train, model_name, timesteps=50000):  \n",
        "    start = time.time()\n",
        "    model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
        "    print('Training time (PPO): ', (end - start) / 60, ' minutes')\n",
        "    return model\n",
        "\n",
        "def train_SAC(env_train, model_name, timesteps=25000):\n",
        "    \"\"\"SAC model\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    model = SAC('MlpPolicy', env_train)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
        "    print('Training time (SAC): ', (end - start) / 60, ' minutes')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGxcH3q2PqNX"
      },
      "source": [
        "def DRL_prediction(df,\n",
        "                   model,\n",
        "                   name,\n",
        "                   last_state,\n",
        "                   iter_num,\n",
        "                   unique_trade_date,\n",
        "                   rebalance_window,\n",
        "                   turbulence_threshold,\n",
        "                   initial):\n",
        "    ### make a prediction based on trained model### \n",
        "\n",
        "    ## trading env\n",
        "    trade_data = data_split(df, start=unique_trade_date[iter_num - rebalance_window], end=unique_trade_date[iter_num])\n",
        "    env_trade = DummyVecEnv([lambda: StockTradeEnv(trade_data,\n",
        "                                                   turbulence_threshold=turbulence_threshold,\n",
        "                                                   initial=initial,\n",
        "                                                   previous_state=last_state,\n",
        "                                                   model_name=name,\n",
        "                                                   iteration=iter_num)])\n",
        "    obs_trade = env_trade.reset()\n",
        "\n",
        "    for i in range(len(trade_data.index.unique())):\n",
        "        action, _states = model.predict(obs_trade)\n",
        "        obs_trade, rewards, dones, info = env_trade.step(action)\n",
        "        if i == (len(trade_data.index.unique()) - 2):\n",
        "            # print(env_test.render())\n",
        "            last_state = env_trade.render()\n",
        "\n",
        "    df_last_state = pd.DataFrame({'last_state': last_state})\n",
        "    df_last_state.to_csv('last_state_{}_{}.csv'.format(name, i), index=False)\n",
        "    return last_state\n",
        "\n",
        "def DRL_validation(model, test_data, test_env, test_obs) -> None:\n",
        "    ###validation process###\n",
        "    for i in range(len(test_data.index.unique())):\n",
        "        action, _states = model.predict(test_obs)\n",
        "        test_obs, rewards, dones, info = test_env.step(action)\n",
        "\n",
        "def get_validation_sharpe(iteration):\n",
        "    ###Calculate Sharpe ratio based on validation results###\n",
        "    df_total_value = pd.read_csv('account_value_validation_{}.csv'.format(iteration), index_col=0)\n",
        "    df_total_value.columns = ['account_value_train']\n",
        "    df_total_value['daily_return'] = df_total_value.pct_change(1)\n",
        "    sharpe = (4 ** 0.5) * df_total_value['daily_return'].mean() / \\\n",
        "             df_total_value['daily_return'].std()\n",
        "    return sharpe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS6NxXMWn4hP"
      },
      "source": [
        "## Ensemble Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvDTnBqFPt4g"
      },
      "source": [
        "def run_ensemble_strategy(df, unique_trade_date, rebalance_window, validation_window) -> None:\n",
        "    \"\"\"Ensemble Strategy that combines PPO, A2C and DDPG\"\"\"\n",
        "    print(\"============Start Ensemble Strategy============\")\n",
        "    # for ensemble model, it's necessary to feed the last state\n",
        "    # of the previous model to the current model as the initial state\n",
        "    last_state_ensemble = []\n",
        "\n",
        "    ppo_sharpe_list = []\n",
        "    ddpg_sharpe_list = []\n",
        "    a2c_sharpe_list = []\n",
        "    td3_sharpe_list = []\n",
        "    sac_sharpe_list = []\n",
        "\n",
        "    model_use = []\n",
        "\n",
        "    # based on the analysis of the in-sample data\n",
        "    #turbulence_threshold = 140\n",
        "    insample_turbulence = df[(df.datadate<20151000) & (df.datadate>=20090000)]\n",
        "    insample_turbulence = insample_turbulence.drop_duplicates(subset=['datadate'])\n",
        "    insample_turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, .90)\n",
        "\n",
        "    start = time.time()\n",
        "    for i in range(rebalance_window + validation_window, len(unique_trade_date), rebalance_window):\n",
        "        print(\"============================================\")\n",
        "        ## initial state is empty\n",
        "        if i - rebalance_window - validation_window == 0:\n",
        "            # inital state\n",
        "            initial = True\n",
        "        else:\n",
        "            # previous state\n",
        "            initial = False\n",
        "\n",
        "        # Tuning trubulence index based on historical data\n",
        "        # Turbulence lookback window is one quarter\n",
        "        historical_turbulence = df[(df.datadate<unique_trade_date[i - rebalance_window - validation_window]) & (df.datadate>=(unique_trade_date[i - rebalance_window - validation_window-63]))]\n",
        "        historical_turbulence = historical_turbulence.drop_duplicates(subset=['datadate'])\n",
        "        historical_turbulence_mean = np.mean(historical_turbulence.turbulence.values)   \n",
        "\n",
        "        if historical_turbulence_mean > insample_turbulence_threshold:\n",
        "            # if the mean of the historical data is greater than the 90% quantile of insample turbulence data\n",
        "            # then we assume that the current market is volatile, \n",
        "            # therefore we set the 90% quantile of insample turbulence data as the turbulence threshold \n",
        "            # meaning the current turbulence can't exceed the 90% quantile of insample turbulence data\n",
        "            turbulence_threshold = insample_turbulence_threshold\n",
        "        else:\n",
        "            # if the mean of the historical data is less than the 90% quantile of insample turbulence data\n",
        "            # then we tune up the turbulence_threshold, meaning we lower the risk \n",
        "            turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 1)\n",
        "        print(\"turbulence_threshold: \", turbulence_threshold)\n",
        "\n",
        "        ############## Environment Setup starts ##############\n",
        "        ## training env\n",
        "        train = data_split(df, start=20090000, end=unique_trade_date[i - rebalance_window - validation_window])\n",
        "        env_train = DummyVecEnv([lambda: StockTrainEnv(train)])\n",
        "\n",
        "        ## validation env\n",
        "        validation = data_split(df, start=unique_trade_date[i - rebalance_window - validation_window],\n",
        "                                end=unique_trade_date[i - rebalance_window])\n",
        "        env_val = DummyVecEnv([lambda: StockValidationEnv(validation,\n",
        "                                                          turbulence_threshold=turbulence_threshold,\n",
        "                                                          iteration=i)])\n",
        "        obs_val = env_val.reset()\n",
        "        ############## Environment Setup ends ##############\n",
        "\n",
        "        ############## Training and Validation starts ##############\n",
        "        print(\"======Model training from: \", 20090000, \"to \",\n",
        "              unique_trade_date[i - rebalance_window - validation_window])\n",
        "        # print(\"training: \",len(data_split(df, start=20090000, end=test.datadate.unique()[i-rebalance_window]) ))\n",
        "        # print(\"==============Model Training===========\")\n",
        "        print(\"======A2C Training========\")\n",
        "        model_a2c = train_A2C(env_train, model_name=\"A2C_30k_dow_{}\".format(i), timesteps=30000)\n",
        "        print(\"======A2C Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
        "              unique_trade_date[i - rebalance_window])\n",
        "        DRL_validation(model=model_a2c, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
        "        sharpe_a2c = get_validation_sharpe(i)\n",
        "        print(\"A2C Sharpe Ratio: \", sharpe_a2c)\n",
        "\n",
        "        print(\"======PPO Training========\")\n",
        "        model_ppo = train_PPO(env_train, model_name=\"PPO_100k_dow_{}\".format(i), timesteps=100000)\n",
        "        print(\"======PPO Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
        "              unique_trade_date[i - rebalance_window])\n",
        "        DRL_validation(model=model_ppo, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
        "        sharpe_ppo = get_validation_sharpe(i)\n",
        "        print(\"PPO Sharpe Ratio: \", sharpe_ppo)\n",
        "\n",
        "        print(\"======TD3 Training========\")\n",
        "        model_td3 = train_TD3(env_train, model_name=\"TD3_30k_dow_{}\".format(i))\n",
        "        print(\"======TD3 Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
        "              unique_trade_date[i - rebalance_window])\n",
        "        DRL_validation(model=model_td3, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
        "        sharpe_td3 = get_validation_sharpe(i)\n",
        "        print(\"TD3 Sharpe Ratio: \", sharpe_td3)\n",
        "\n",
        "        print(\"======SAC Training========\")\n",
        "        model_sac = train_SAC(env_train, model_name=\"SAC_30k_dow_{}\".format(i))\n",
        "        print(\"======SAC Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
        "              unique_trade_date[i - rebalance_window])\n",
        "        DRL_validation(model=model_sac, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
        "        sharpe_sac = get_validation_sharpe(i)\n",
        "        print(\"SAC Sharpe Ratio: \", sharpe_sac)\n",
        "\n",
        "        print(\"======DDPG Training========\")\n",
        "        model_ddpg = train_DDPG(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=10000)\n",
        "        #model_ddpg = train_TD3(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=20000)\n",
        "        print(\"======DDPG Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
        "              unique_trade_date[i - rebalance_window])\n",
        "        DRL_validation(model=model_ddpg, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
        "        sharpe_ddpg = get_validation_sharpe(i)\n",
        "\n",
        "        ppo_sharpe_list.append(sharpe_ppo)\n",
        "        a2c_sharpe_list.append(sharpe_a2c)\n",
        "        ddpg_sharpe_list.append(sharpe_ddpg)\n",
        "        td3_sharpe_list.append(sharpe_td3)\n",
        "        sac_sharpe_list.append(sharpe_sac)\n",
        "\n",
        "        # Model Selection based on sharpe ratio\n",
        "        # if (sharpe_ppo >= sharpe_a2c) & (sharpe_ppo >= sharpe_ddpg):\n",
        "        #     model_ensemble = model_ppo\n",
        "        #     model_use.append('PPO')\n",
        "        # elif (sharpe_a2c > sharpe_ppo) & (sharpe_a2c > sharpe_ddpg):\n",
        "        #     model_ensemble = model_a2c\n",
        "        #     model_use.append('A2C')\n",
        "        # else:\n",
        "        #     model_ensemble = model_ddpg\n",
        "        #     model_use.append('DDPG')\n",
        "\n",
        "        all_sharpes= [ sharpe_a2c  ,sharpe_td3 ,sharpe_sac, sharpe_ddpg, sharpe_ppo ]\n",
        "        names_sharpes =[\n",
        "                        (model_a2c,'A2C'),             \n",
        "                        (model_td3,'TD3'),\n",
        "                        (model_sac,'SAC'),\n",
        "                        (model_ddpg, 'DDPG'),\n",
        "                        (model_ppo, 'PPO')         \n",
        "                        ]\n",
        "        MAX_SHARPE=all_sharpes.index(max(all_sharpes))\n",
        "        model_ensemble=names_sharpes[MAX_SHARPE][0]\n",
        "        model_use.append(names_sharpes[MAX_SHARPE][1])\n",
        "\n",
        "        print('*'*50)\n",
        "        print('picked up technique',names_sharpes[MAX_SHARPE][0])\n",
        "        ############## Training and Validation ends ##############    \n",
        "\n",
        "        ############## Trading starts ##############    \n",
        "        print(\"======Trading from: \", unique_trade_date[i - rebalance_window], \"to \", unique_trade_date[i])\n",
        "        #print(\"Used Model: \", model_ensemble)\n",
        "        last_state_ensemble = DRL_prediction(df=df, model=model_ensemble, name=\"ensemble\",\n",
        "                                             last_state=last_state_ensemble, iter_num=i,\n",
        "                                             unique_trade_date=unique_trade_date,\n",
        "                                             rebalance_window=rebalance_window,\n",
        "                                             turbulence_threshold=turbulence_threshold,\n",
        "                                             initial=initial)\n",
        "        # print(\"============Trading Done============\")\n",
        "        ############## Trading ends ##############    \n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Ensemble Strategy took: \", (end - start) / 60, \" minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge1acs5WQq1w",
        "outputId": "5ffaa8bf-e07b-4b71-e92a-b438ab5184be"
      },
      "source": [
        "def run_model() -> None:\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "\n",
        "    # read and preprocess data\n",
        "    data = preprocess_data()\n",
        "    data = add_turbulence(data)\n",
        "\n",
        "    # 2015/10/01 is the date that validation starts\n",
        "    # 2016/01/01 is the date that real trading starts\n",
        "    # unique_trade_date needs to start from 2015/10/01 for validation purpose\n",
        "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
        "\n",
        "    # rebalance_window is the number of months to retrain the model\n",
        "    # validation_window is the number of months to validation the model and select for trading\n",
        "    rebalance_window = 63\n",
        "    validation_window = 63\n",
        "    \n",
        "    ## Ensemble Strategy\n",
        "    run_ensemble_strategy(df=data, \n",
        "                          unique_trade_date= unique_trade_date,\n",
        "                          rebalance_window = rebalance_window,\n",
        "                          validation_window=validation_window)\n",
        "\n",
        "    #_logger.info(f\"saving model version: {_version}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "---------------------------------\n",
            "| explained_variance | -0.0218  |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 17       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 3.73     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0535   |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 18.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 0.256    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 5.03     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 332       |\n",
            "| nupdates           | 5200      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 26000     |\n",
            "| value_loss         | 51.8      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 33.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.246   |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 5.67     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.129   |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 0.987    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 333      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 19       |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 333       |\n",
            "| nupdates           | 5700      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 28500     |\n",
            "| value_loss         | 0.944     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 333      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 7.68     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 333      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 30       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 333      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 1.52     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.510227632522583  minutes\n",
            "======A2C Validation from:  20171003 to  20180103\n",
            "A2C Sharpe Ratio:  0.5052959486800095\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.130888907114665  minutes\n",
            "======PPO Validation from:  20171003 to  20180103\n",
            "PPO Sharpe Ratio:  0.32954617889000665\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.4688464879989622  minutes\n",
            "======TD3 Validation from:  20171003 to  20180103\n",
            "TD3 Sharpe Ratio:  0.5833137260553128\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.657357664903005  minutes\n",
            "======SAC Validation from:  20171003 to  20180103\n",
            "SAC Sharpe Ratio:  0.5237791696630193\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9074219902356465  minutes\n",
            "======DDPG Validation from:  20171003 to  20180103\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.td3.td3.TD3 object at 0x7f3be5337320>\n",
            "======Trading from:  20180103 to  20180405\n",
            "previous_total_asset:1312226.802532669\n",
            "end_total_asset:1291307.2495667671\n",
            "total_reward:-20919.552965901792\n",
            "total_cost:  2526.807601067023\n",
            "total trades:  255\n",
            "Sharpe:  -0.06492787178464814\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358324\n",
            "======Model training from:  20090000 to  20180103\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0.0261   |\n",
            "| fps                     | 23       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 4.05     |\n",
            "| reference_Q_std         | 3.76     |\n",
            "| reference_action_mean   | -0.0652  |\n",
            "| reference_action_std    | 0.972    |\n",
            "| reference_actor_Q_mean  | 4.73     |\n",
            "| reference_actor_Q_std   | 3.71     |\n",
            "| rollout/Q_mean          | 2.06     |\n",
            "| rollout/actions_mean    | 0.0248   |\n",
            "| rollout/actions_std     | 0.817    |\n",
            "| rollout/episode_steps   | 2.2e+03  |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 286      |\n",
            "| rollout/return_history  | 286      |\n",
            "| total/duration          | 53.7     |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 186      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.38    |\n",
            "| train/loss_critic       | 2.34     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.0775   |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0887   |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.294    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.197   |\n",
            "| fps                | 308      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 2.14     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.1     |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 6.55     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.175    |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 15.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0011   |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 1.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.274   |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 67.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0635   |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 2.17     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0682   |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 12.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0158  |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 4.41     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0003   |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 3.37     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.432   |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 4.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 44.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -4.35    |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 0.353    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00234  |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 4.16     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 321       |\n",
            "| nupdates           | 1500      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 7500      |\n",
            "| value_loss         | 26.9      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0195  |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 30.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.174   |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 63.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.312    |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 15.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.117   |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 0.424    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.307   |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 2.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.5     |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 6.44     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.152    |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 6.84     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.42    |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 2.79     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.626   |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 25.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 2.86     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0159  |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 217      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00184 |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 1.65     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 2.38e-07 |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 7.36     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.396   |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 1.08     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.157   |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 39.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 18.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.267    |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 2.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 1.35     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.171   |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 6.02     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 328       |\n",
            "| nupdates           | 3500      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 17500     |\n",
            "| value_loss         | 7.15      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 5.32     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.451   |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 16.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.153   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 6.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 2.59     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 328       |\n",
            "| nupdates           | 4000      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 20000     |\n",
            "| value_loss         | 10.6      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.165   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 1.34     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 2.53     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 8.43     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.114   |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 0.56     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 0.474    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.108   |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 9.41     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 9.95e-06 |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 3.98     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 329       |\n",
            "| nupdates           | 4800      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 24000     |\n",
            "| value_loss         | 1.93      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.103   |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 5.44     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.375   |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 3.04     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 9.54e-07 |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 0.636    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0478  |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 18.9     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 330       |\n",
            "| nupdates           | 5300      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 26500     |\n",
            "| value_loss         | 2.29      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 0.697    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00801  |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 0.395    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0763   |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 3.14     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0248  |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 5.51     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 7.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 6.76     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.136    |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 2.05     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.5228888511657714  minutes\n",
            "======A2C Validation from:  20180103 to  20180405\n",
            "A2C Sharpe Ratio:  -0.023742044847170587\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.163389845689138  minutes\n",
            "======PPO Validation from:  20180103 to  20180405\n",
            "PPO Sharpe Ratio:  -0.02873476086950317\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.5268343369166057  minutes\n",
            "======TD3 Validation from:  20180103 to  20180405\n",
            "TD3 Sharpe Ratio:  -0.07002136848149067\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.6693676988283794  minutes\n",
            "======SAC Validation from:  20180103 to  20180405\n",
            "SAC Sharpe Ratio:  -0.09310436467328936\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9130954186121623  minutes\n",
            "======DDPG Validation from:  20180103 to  20180405\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.ddpg.ddpg.DDPG object at 0x7f3be9d0dcf8>\n",
            "======Trading from:  20180405 to  20180705\n",
            "previous_total_asset:1291307.2495667671\n",
            "end_total_asset:1299919.262826607\n",
            "total_reward:8612.013259839965\n",
            "total_cost:  2840.681595436007\n",
            "total trades:  735\n",
            "Sharpe:  0.04468380282780212\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358324\n",
            "======Model training from:  20090000 to  20180405\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 24       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 2.36     |\n",
            "| reference_Q_std         | 2.33     |\n",
            "| reference_action_mean   | -0.00267 |\n",
            "| reference_action_std    | 0.951    |\n",
            "| reference_actor_Q_mean  | 3.14     |\n",
            "| reference_actor_Q_std   | 2.18     |\n",
            "| rollout/Q_mean          | 1.54     |\n",
            "| rollout/actions_mean    | 0.0524   |\n",
            "| rollout/actions_std     | 0.826    |\n",
            "| rollout/episode_steps   | 2.27e+03 |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 173      |\n",
            "| rollout/return_history  | 173      |\n",
            "| total/duration          | 54.1     |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 185      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -3.22    |\n",
            "| train/loss_critic       | 0.963    |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.0465   |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.3     |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.738    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00365 |\n",
            "| fps                | 293      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.96     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.231   |\n",
            "| fps                | 299      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 9        |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.24    |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 14.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 3.03     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0304   |\n",
            "| fps                | 305      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 8.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.122   |\n",
            "| fps                | 308      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 6.32     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0735   |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 177      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.163   |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 6.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.261    |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 4.12     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.131    |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 8.48     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0108   |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 0.626    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.254   |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 20       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 0.114    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.217    |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 1.08     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.26    |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 1.34     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0977   |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 8.69     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0969  |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 3.44     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.106   |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 3.2      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0255  |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 5.42     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 19.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 3.73     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 3.35     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0256   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 7        |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 4.06     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 5.56     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.196   |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 9.86     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 323       |\n",
            "| nupdates           | 2800      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 14000     |\n",
            "| value_loss         | 2.38      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.191    |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 7.13     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 3.77     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 35.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 6.7      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.177   |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 11.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.13     |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 1.7      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 1.72     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.133    |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 3.19     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 3.81     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0847  |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 2.96     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 3.09     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0561  |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 2.29     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 1.33     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 9.61     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0861  |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 4.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0187  |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 1.88     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.565    |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 6.83     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.176   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 4.09     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 4.9      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.352   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 8.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 4.73     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 2.88     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.223   |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 0.273    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00447  |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 40.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 30       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 7.01     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.321   |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 27.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.518    |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 1.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.115    |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 6.79     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 3.56     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 1.99     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.565   |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 5.81     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.5211013952891033  minutes\n",
            "======A2C Validation from:  20180405 to  20180705\n",
            "A2C Sharpe Ratio:  -0.15121300272187918\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.182674129803975  minutes\n",
            "======PPO Validation from:  20180405 to  20180705\n",
            "PPO Sharpe Ratio:  -0.17605949802443335\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.50483668645223  minutes\n",
            "======TD3 Validation from:  20180405 to  20180705\n",
            "TD3 Sharpe Ratio:  0.017292183802479676\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.681782368818919  minutes\n",
            "======SAC Validation from:  20180405 to  20180705\n",
            "SAC Sharpe Ratio:  -0.01085579037245971\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9103623946507772  minutes\n",
            "======DDPG Validation from:  20180405 to  20180705\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.td3.td3.TD3 object at 0x7f3be54297f0>\n",
            "======Trading from:  20180705 to  20181003\n",
            "previous_total_asset:1299919.262826607\n",
            "end_total_asset:1318463.1051066082\n",
            "total_reward:18543.842280001147\n",
            "total_cost:  5278.973400000002\n",
            "total trades:  684\n",
            "Sharpe:  0.14976890174102256\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358324\n",
            "======Model training from:  20090000 to  20180705\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 24       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 2.37     |\n",
            "| reference_Q_std         | 2.12     |\n",
            "| reference_action_mean   | -0.234   |\n",
            "| reference_action_std    | 0.945    |\n",
            "| reference_actor_Q_mean  | 2.97     |\n",
            "| reference_actor_Q_std   | 2.1      |\n",
            "| rollout/Q_mean          | 1.49     |\n",
            "| rollout/actions_mean    | -0.13    |\n",
            "| rollout/actions_std     | 0.805    |\n",
            "| rollout/episode_steps   | 2.33e+03 |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 144      |\n",
            "| rollout/return_history  | 144      |\n",
            "| total/duration          | 53.9     |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 185      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -2.88    |\n",
            "| train/loss_critic       | 0.921    |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.112    |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.48     |\n",
            "| fps                | 290      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.0908   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.354   |\n",
            "| fps                | 309      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 2.7      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0681   |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 7.17     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0784  |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 22.5     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 315       |\n",
            "| nupdates           | 500       |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 2500      |\n",
            "| value_loss         | 0.106     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.446    |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 3.23     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.578   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 0.804    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00888  |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 14.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0675   |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 7.73     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.123    |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 15.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.315   |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 10.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.127    |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 1.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0137  |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 67       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.134    |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 1.44     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.154   |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 30.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00544  |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 28.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 4.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.449    |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 4.12     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 326       |\n",
            "| nupdates           | 1900      |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 9500      |\n",
            "| value_loss         | 21.1      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 0.863    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 11.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 8.82     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 6.55e-05 |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 2.56     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 16.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0431   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 15.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 15       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0577   |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 22.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0657   |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 1        |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 329       |\n",
            "| nupdates           | 2900      |\n",
            "| policy_entropy     | 42.7      |\n",
            "| total_timesteps    | 14500     |\n",
            "| value_loss         | 8.81      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0234   |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 63.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0589  |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 4.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 2.67     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0765   |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 2.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 7.01     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.015    |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 4.16     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 4.46     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 59.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 333      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 2.08     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.147    |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 10.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 333      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 18       |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000126 |\n",
            "| fps                | 333       |\n",
            "| nupdates           | 4100      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 20500     |\n",
            "| value_loss         | 2.91      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.975   |\n",
            "| fps                | 334      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 1.5      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 334      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 6.48     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0112   |\n",
            "| fps                | 334      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 27.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00882 |\n",
            "| fps                | 334      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 22.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 335      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 6.35     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 335      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 22.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0888   |\n",
            "| fps                | 335      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 10.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 335      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 9.22     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 336      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 3.21     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 336      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 13       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 337      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 0.0593   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 336      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 6.1      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 337       |\n",
            "| nupdates           | 5400      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 27000     |\n",
            "| value_loss         | 17.6      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 337      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 1.27     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 337      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 4.08     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00136  |\n",
            "| fps                | 338      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 4.59     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 337      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 3.46     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 338      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 1.47     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 338      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 0.615    |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.4884114980697631  minutes\n",
            "======A2C Validation from:  20180705 to  20181003\n",
            "A2C Sharpe Ratio:  0.09603620268308329\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.258164982000987  minutes\n",
            "======PPO Validation from:  20180705 to  20181003\n",
            "PPO Sharpe Ratio:  0.23545583221671146\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.4981302976608277  minutes\n",
            "======TD3 Validation from:  20180705 to  20181003\n",
            "TD3 Sharpe Ratio:  0.0911188612835974\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.7135350624720256  minutes\n",
            "======SAC Validation from:  20180705 to  20181003\n",
            "SAC Sharpe Ratio:  0.3586037944934339\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9143737991650899  minutes\n",
            "======DDPG Validation from:  20180705 to  20181003\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.sac.sac.SAC object at 0x7f3be7d24c18>\n",
            "======Trading from:  20181003 to  20190104\n",
            "previous_total_asset:1318463.1051066082\n",
            "end_total_asset:1337008.0121066088\n",
            "total_reward:18544.90700000059\n",
            "total_cost:  1659.104\n",
            "total trades:  189\n",
            "Sharpe:  0.19036216776594542\n",
            "============================================\n",
            "turbulence_threshold:  171.09407156310422\n",
            "======Model training from:  20090000 to  20181003\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 23       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 4.14     |\n",
            "| reference_Q_std         | 3.66     |\n",
            "| reference_action_mean   | -0.128   |\n",
            "| reference_action_std    | 0.952    |\n",
            "| reference_actor_Q_mean  | 5.37     |\n",
            "| reference_actor_Q_std   | 3.72     |\n",
            "| rollout/Q_mean          | 2.09     |\n",
            "| rollout/actions_mean    | -0.0757  |\n",
            "| rollout/actions_std     | 0.804    |\n",
            "| rollout/episode_steps   | 2.39e+03 |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 335      |\n",
            "| rollout/return_history  | 335      |\n",
            "| total/duration          | 54       |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 185      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.78    |\n",
            "| train/loss_critic       | 2.81     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.222    |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0516   |\n",
            "| fps                | 276      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.588    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0225   |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.59     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0846  |\n",
            "| fps                | 292      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 5.85     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0833   |\n",
            "| fps                | 296      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 17.9     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 295       |\n",
            "| nupdates           | 500       |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 2500      |\n",
            "| value_loss         | 1.15      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 296      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 2.77     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0164  |\n",
            "| fps                | 298      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 5.27     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.12    |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 5.58     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000337 |\n",
            "| fps                | 303       |\n",
            "| nupdates           | 900       |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 4500      |\n",
            "| value_loss         | 16.9      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 303       |\n",
            "| nupdates           | 1000      |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 5000      |\n",
            "| value_loss         | 5.26      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0863   |\n",
            "| fps                | 305      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 0.293    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 308      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 5.79     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.352   |\n",
            "| fps                | 310      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 0.718    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.221    |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 0.444    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 311       |\n",
            "| nupdates           | 1500      |\n",
            "| policy_entropy     | 42.7      |\n",
            "| total_timesteps    | 7500      |\n",
            "| value_loss         | 5.6       |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0049  |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 9.63     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 4.74     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0869   |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 2.19     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00138  |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 1.7      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.132    |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 3.86     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0105  |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 6.43     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 4.98     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 319       |\n",
            "| nupdates           | 2300      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 11500     |\n",
            "| value_loss         | 6.86      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 0.835    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0496   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 0.964    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.12     |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 3.59     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.324   |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 2.65     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0331  |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 9.49     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 4.24     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.643    |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 6.33     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 3.83     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00542  |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 0.659    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.29e-05 |\n",
            "| fps                | 324       |\n",
            "| nupdates           | 3300      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 16500     |\n",
            "| value_loss         | 4.06      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 1.34     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 1.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 4.1      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 1.15     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 4.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0902  |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 3.36     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 16.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 40.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 2.24e-05 |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 2.11     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.194    |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 0.308    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 3.51     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.406    |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 2.87     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 0.696    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00965 |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 9.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 38       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 2.48     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 1.67     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 8.94e-07 |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 1.82     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.000269 |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 4.2      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 5.75     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 331       |\n",
            "| nupdates           | 5400      |\n",
            "| policy_entropy     | 43.4      |\n",
            "| total_timesteps    | 27000     |\n",
            "| value_loss         | 13.6      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.102   |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 0.0547   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 331       |\n",
            "| nupdates           | 5600      |\n",
            "| policy_entropy     | 43.3      |\n",
            "| total_timesteps    | 28000     |\n",
            "| value_loss         | 1.12      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 31.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 1.31     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 28.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.615   |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 2.53     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.518391199906667  minutes\n",
            "======A2C Validation from:  20181003 to  20190104\n",
            "A2C Sharpe Ratio:  -0.39945853066379666\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.156188996632894  minutes\n",
            "======PPO Validation from:  20181003 to  20190104\n",
            "PPO Sharpe Ratio:  -0.3892465471054034\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.5226229270299276  minutes\n",
            "======TD3 Validation from:  20181003 to  20190104\n",
            "TD3 Sharpe Ratio:  -0.29069902063521486\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.6856348117192588  minutes\n",
            "======SAC Validation from:  20181003 to  20190104\n",
            "SAC Sharpe Ratio:  -0.25913048235922026\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9108130097389221  minutes\n",
            "======DDPG Validation from:  20181003 to  20190104\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.sac.sac.SAC object at 0x7f3bec68b6d8>\n",
            "======Trading from:  20190104 to  20190405\n",
            "previous_total_asset:1337008.0121066088\n",
            "end_total_asset:1432642.9046666084\n",
            "total_reward:95634.8925599996\n",
            "total_cost:  1725.8483499999998\n",
            "total trades:  965\n",
            "Sharpe:  0.3781701023045149\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358324\n",
            "======Model training from:  20090000 to  20190104\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 23       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 3.8      |\n",
            "| reference_Q_std         | 2.86     |\n",
            "| reference_action_mean   | -0.227   |\n",
            "| reference_action_std    | 0.94     |\n",
            "| reference_actor_Q_mean  | 4.52     |\n",
            "| reference_actor_Q_std   | 3        |\n",
            "| rollout/Q_mean          | 2.76     |\n",
            "| rollout/actions_mean    | -0.151   |\n",
            "| rollout/actions_std     | 0.802    |\n",
            "| rollout/episode_steps   | 2.46e+03 |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 250      |\n",
            "| rollout/return_history  | 250      |\n",
            "| total/duration          | 53.9     |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 185      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.57    |\n",
            "| train/loss_critic       | 1.57     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.134    |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0621   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 1.41     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.141   |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.68     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.604   |\n",
            "| fps                | 308      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 7.01     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0338   |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 18.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0104  |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 92.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.178   |\n",
            "| fps                | 310      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 7.68     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 312       |\n",
            "| nupdates           | 700       |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 3500      |\n",
            "| value_loss         | 1.62      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 2.73     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0881  |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 9.32     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00811 |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 12.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.222   |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 10.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0474  |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 3.76     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.037    |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 15.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.396    |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 0.215    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00241  |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 48.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0268   |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 4.91     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0864   |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 0.829    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0206   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 8.63     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0921  |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 6        |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 29.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0648  |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 4.16     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.178    |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 7.67     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 13.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 9.83     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 3.42     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 323       |\n",
            "| nupdates           | 2600      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 13000     |\n",
            "| value_loss         | 2.69      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 0.55     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 72.9     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 325       |\n",
            "| nupdates           | 2900      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 14500     |\n",
            "| value_loss         | 1.63      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00627 |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 6.56     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 3.96e-05 |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 21.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.189   |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 13.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -2.77    |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 6.65     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 14.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 10.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.11     |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 39.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 5.15     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.262    |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 2.69     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.16    |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 51.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0647   |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 15.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0529   |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 3.88     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 1.49     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0958  |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 7.69     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.167    |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 8.02     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 36.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0743  |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 63.3     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 327       |\n",
            "| nupdates           | 4700      |\n",
            "| policy_entropy     | 43.5      |\n",
            "| total_timesteps    | 23500     |\n",
            "| value_loss         | 7.07      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 1.7      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 328       |\n",
            "| nupdates           | 4900      |\n",
            "| policy_entropy     | 43.6      |\n",
            "| total_timesteps    | 24500     |\n",
            "| value_loss         | 25.7      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 329       |\n",
            "| nupdates           | 5000      |\n",
            "| policy_entropy     | 43.6      |\n",
            "| total_timesteps    | 25000     |\n",
            "| value_loss         | 16        |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0832   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.6     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 2.23     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -3.58e-07 |\n",
            "| fps                | 329       |\n",
            "| nupdates           | 5200      |\n",
            "| policy_entropy     | 43.6      |\n",
            "| total_timesteps    | 26000     |\n",
            "| value_loss         | 20        |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.7     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 3.82     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 330       |\n",
            "| nupdates           | 5400      |\n",
            "| policy_entropy     | 43.8      |\n",
            "| total_timesteps    | 27000     |\n",
            "| value_loss         | 5.27      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 6.56e-07 |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.7     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 203      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.7     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 1.23     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.8     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 13.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.8     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 17.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.9     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 88.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.9     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 287      |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.5188625613848368  minutes\n",
            "======A2C Validation from:  20190104 to  20190405\n",
            "A2C Sharpe Ratio:  0.03473339843649882\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.15832835038503  minutes\n",
            "======PPO Validation from:  20190104 to  20190405\n",
            "PPO Sharpe Ratio:  0.044578716806497234\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.5413293759028117  minutes\n",
            "======TD3 Validation from:  20190104 to  20190405\n",
            "TD3 Sharpe Ratio:  0.07745192638239076\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.675263885656993  minutes\n",
            "======SAC Validation from:  20190104 to  20190405\n",
            "SAC Sharpe Ratio:  -0.027788916419381946\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9261032978693644  minutes\n",
            "======DDPG Validation from:  20190104 to  20190405\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.td3.td3.TD3 object at 0x7f3be076c390>\n",
            "======Trading from:  20190405 to  20190708\n",
            "previous_total_asset:1432642.9046666084\n",
            "end_total_asset:1440104.0104159007\n",
            "total_reward:7461.105749292299\n",
            "total_cost:  1691.36330711371\n",
            "total trades:  144\n",
            "Sharpe:  0.21770139588188828\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358324\n",
            "======Model training from:  20090000 to  20190405\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 24       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 4.76     |\n",
            "| reference_Q_std         | 3.14     |\n",
            "| reference_action_mean   | -0.0162  |\n",
            "| reference_action_std    | 0.983    |\n",
            "| reference_actor_Q_mean  | 5.94     |\n",
            "| reference_actor_Q_std   | 3.2      |\n",
            "| rollout/Q_mean          | 2.92     |\n",
            "| rollout/actions_mean    | -0.00833 |\n",
            "| rollout/actions_std     | 0.796    |\n",
            "| rollout/episode_steps   | 2.52e+03 |\n",
            "| rollout/episodes        | 3        |\n",
            "| rollout/return          | 154      |\n",
            "| rollout/return_history  | 154      |\n",
            "| total/duration          | 54.8     |\n",
            "| total/episodes          | 3        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 182      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -5.62    |\n",
            "| train/loss_critic       | 1.77     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.215    |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.339    |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.114    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0325   |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.66     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.232   |\n",
            "| fps                | 306      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 5.78     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0564   |\n",
            "| fps                | 310      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 19.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0538   |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 86.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.11     |\n",
            "| fps                | 310      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 6.53     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.787   |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 1.21     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.197    |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 2        |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.138   |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 0.462    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.099   |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 9.56     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.118   |\n",
            "| fps                | 310      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 34       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.12    |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 0.78     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 4.77e-07 |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 0.744    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.087    |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 8.26     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 9.35     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 2.12     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 3.47     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 7.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 8.22     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 0.329    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 1.95     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0283  |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 4.57     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 0.638    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.022    |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 23.3     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 323       |\n",
            "| nupdates           | 2500      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 12500     |\n",
            "| value_loss         | 6.17      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.000222 |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 1.62     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 2.98e-07 |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 23.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 3.49     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 11.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 8.58     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 0.161    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0212   |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 3.44     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0582  |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 4.47     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 41.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 0.828    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 61.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0176  |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 13.7     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 328       |\n",
            "| nupdates           | 3800      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 19000     |\n",
            "| value_loss         | 80        |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 36.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 20.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 384      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0288  |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 14.7     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 329       |\n",
            "| nupdates           | 4300      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 21500     |\n",
            "| value_loss         | 35.4      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 14.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 56.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 764      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -9.07e-05 |\n",
            "| fps                | 330       |\n",
            "| nupdates           | 4700      |\n",
            "| policy_entropy     | 43.2      |\n",
            "| total_timesteps    | 23500     |\n",
            "| value_loss         | 6.1       |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 34       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 4.67     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 18.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 5.42     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0776  |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 5.31     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00886 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 6.19     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 4.23     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 7.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00447  |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 9.6      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 331       |\n",
            "| nupdates           | 5700      |\n",
            "| policy_entropy     | 43.3      |\n",
            "| total_timesteps    | 28500     |\n",
            "| value_loss         | 1.55      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00913 |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 9.72     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 1.76     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 332      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 6.78     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.5152294556299846  minutes\n",
            "======A2C Validation from:  20190405 to  20190708\n",
            "A2C Sharpe Ratio:  0.25854358256443566\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.247085241476695  minutes\n",
            "======PPO Validation from:  20190405 to  20190708\n",
            "PPO Sharpe Ratio:  0.2318219612621426\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.4855885108311973  minutes\n",
            "======TD3 Validation from:  20190405 to  20190708\n",
            "TD3 Sharpe Ratio:  0.11848679396116332\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.6924341400464376  minutes\n",
            "======SAC Validation from:  20190405 to  20190708\n",
            "SAC Sharpe Ratio:  0.41645141077582154\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.927237351735433  minutes\n",
            "======DDPG Validation from:  20190405 to  20190708\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.sac.sac.SAC object at 0x7f3be559b9e8>\n",
            "======Trading from:  20190708 to  20191004\n",
            "previous_total_asset:1440104.0104159007\n",
            "end_total_asset:1442821.0317759013\n",
            "total_reward:2717.0213600005955\n",
            "total_cost:  2481.35862\n",
            "total trades:  272\n",
            "Sharpe:  0.03039698069987065\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358324\n",
            "======Model training from:  20090000 to  20190708\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 1.19e-07 |\n",
            "| fps                     | 25       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 4.68     |\n",
            "| reference_Q_std         | 3.19     |\n",
            "| reference_action_mean   | 0.0673   |\n",
            "| reference_action_std    | 0.972    |\n",
            "| reference_actor_Q_mean  | 5.36     |\n",
            "| reference_actor_Q_std   | 3.26     |\n",
            "| rollout/Q_mean          | 2.49     |\n",
            "| rollout/actions_mean    | 0.0707   |\n",
            "| rollout/actions_std     | 0.805    |\n",
            "| rollout/episode_steps   | 2.58e+03 |\n",
            "| rollout/episodes        | 3        |\n",
            "| rollout/return          | 205      |\n",
            "| rollout/return_history  | 205      |\n",
            "| total/duration          | 54.9     |\n",
            "| total/episodes          | 3        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 182      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.44    |\n",
            "| train/loss_critic       | 1.72     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.114    |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0541  |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.518    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.156    |\n",
            "| fps                | 299      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.302    |\n",
            "| fps                | 304      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 4.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 307      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 10.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0258   |\n",
            "| fps                | 308      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 56.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00891  |\n",
            "| fps                | 305      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 2.04     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0521   |\n",
            "| fps                | 308      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 0.529    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.221   |\n",
            "| fps                | 309      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 1.12     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.266   |\n",
            "| fps                | 310      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 3.89     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.146    |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 13.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0781  |\n",
            "| fps                | 310      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 1.6      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.000663 |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 17.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.128   |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 0.192    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00101 |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 1.84     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0752  |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 6.06     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00593  |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 14.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0017  |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 0.644    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.22     |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 0.165    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 8.88     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00722  |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 1.15     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0313  |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 11.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.205   |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 8.2      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.24    |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 3.56     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 1.73     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 319       |\n",
            "| nupdates           | 2500      |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 12500     |\n",
            "| value_loss         | 3.67      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0272   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 54.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0352  |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 0.738    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 10.2     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 321       |\n",
            "| nupdates           | 2900      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 14500     |\n",
            "| value_loss         | 6.77      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 44.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 38       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 20.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.117    |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 0.756    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.48    |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 9.19     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 2.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 2.37     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 17.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 0.234    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 5.82     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 327       |\n",
            "| nupdates           | 4000      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 20000     |\n",
            "| value_loss         | 4.61      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.386   |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 0.432    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 47       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00459  |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 7.91     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 12.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.485   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 25.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.031   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 9.21     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.105   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 47.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.113    |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 3.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.651   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 9.04     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000307 |\n",
            "| fps                | 328       |\n",
            "| nupdates           | 5000      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 25000     |\n",
            "| value_loss         | 10.5      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 3.75     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 10.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.197    |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 7.12     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0585   |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 1.69     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 1.97     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0144  |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 9.31     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 1.41     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.112    |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 0.599    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0142  |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 4.09     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 329       |\n",
            "| nupdates           | 6000      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 30000     |\n",
            "| value_loss         | 1.27      |\n",
            "----------------------------------\n",
            "Training time (A2C):  1.5279955903689066  minutes\n",
            "======A2C Validation from:  20190708 to  20191004\n",
            "A2C Sharpe Ratio:  0.22837457948984002\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.289909152189891  minutes\n",
            "======PPO Validation from:  20190708 to  20191004\n",
            "PPO Sharpe Ratio:  -0.03469010136176961\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.551847279071808  minutes\n",
            "======TD3 Validation from:  20190708 to  20191004\n",
            "TD3 Sharpe Ratio:  0.16124919212483257\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.69577116171519  minutes\n",
            "======SAC Validation from:  20190708 to  20191004\n",
            "SAC Sharpe Ratio:  -0.06858008939788217\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.932357128461202  minutes\n",
            "======DDPG Validation from:  20190708 to  20191004\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.a2c.a2c.A2C object at 0x7f3bed88ee48>\n",
            "======Trading from:  20191004 to  20200106\n",
            "previous_total_asset:1442821.0317759013\n",
            "end_total_asset:1441536.0083776521\n",
            "total_reward:-1285.0233982491773\n",
            "total_cost:  274.8732644301891\n",
            "total trades:  66\n",
            "Sharpe:  -0.32899633795029937\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358324\n",
            "======Model training from:  20090000 to  20191004\n",
            "======A2C Training========\n",
            "---------------------------------------\n",
            "| explained_variance      | -1.19e-07 |\n",
            "| fps                     | 24        |\n",
            "| nupdates                | 1         |\n",
            "| policy_entropy          | 42.6      |\n",
            "| reference_Q_mean        | 4.49      |\n",
            "| reference_Q_std         | 2.91      |\n",
            "| reference_action_mean   | 0.0568    |\n",
            "| reference_action_std    | 0.973     |\n",
            "| reference_actor_Q_mean  | 5.22      |\n",
            "| reference_actor_Q_std   | 2.97      |\n",
            "| rollout/Q_mean          | 2.71      |\n",
            "| rollout/actions_mean    | -0.0155   |\n",
            "| rollout/actions_std     | 0.817     |\n",
            "| rollout/episode_steps   | 2.64e+03  |\n",
            "| rollout/episodes        | 3         |\n",
            "| rollout/return          | 201       |\n",
            "| rollout/return_history  | 201       |\n",
            "| total/duration          | 55.2      |\n",
            "| total/episodes          | 3         |\n",
            "| total/epochs            | 1         |\n",
            "| total/steps             | 9998      |\n",
            "| total/steps_per_second  | 181       |\n",
            "| total_timesteps         | 5         |\n",
            "| train/loss_actor        | -4.81     |\n",
            "| train/loss_critic       | 1.87      |\n",
            "| train/param_noise_di... | 0         |\n",
            "| value_loss              | 0.0577    |\n",
            "---------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -2.65    |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.716    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0631   |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.6      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.111    |\n",
            "| fps                | 309      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 5.04     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0153  |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 12.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0135  |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 99.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.518    |\n",
            "| fps                | 309      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 1.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.418    |\n",
            "| fps                | 310      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 0.263    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.274   |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 2.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0875  |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 33.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 38.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 4.37     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.136   |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 3.57     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.8     |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 2.79     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00478 |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 5.3      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.000238 |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 6.26     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 1.74     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0603  |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 8.01     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 2.55     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.24     |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 0.475    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.1     |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 6.52     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.421   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 7.65     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.456    |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 17.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00828 |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 9.53     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00233  |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 6.11     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0583   |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 425      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.045   |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 13.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 11.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0781   |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 7.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.17    |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 2.55     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00947  |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 23       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0278  |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 48.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0685   |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 22       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 0.054    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -8.34e-07 |\n",
            "| fps                | 325       |\n",
            "| nupdates           | 3400      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 17000     |\n",
            "| value_loss         | 1.05      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 2.38e-07 |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 8.97     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.289    |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 4.34     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 0.535    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.946   |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 31.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 3.98     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 1.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 327      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 4.58     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0564  |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 0.764    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 328       |\n",
            "| nupdates           | 4300      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 21500     |\n",
            "| value_loss         | 3.71      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.131   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 8.34     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.222   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 21.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0365   |\n",
            "| fps                | 328      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 5.13     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 12.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 61.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0963   |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 24       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 35       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 329      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 28       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 2.18     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 330       |\n",
            "| nupdates           | 5300      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 26500     |\n",
            "| value_loss         | 11.9      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 9.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0282  |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 0.829    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 330      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 21.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 7.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 0.867    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 3.71     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 331      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 2.69     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.5193001786867777  minutes\n",
            "======A2C Validation from:  20191004 to  20200106\n",
            "A2C Sharpe Ratio:  0.001786545385946055\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.215203348795573  minutes\n",
            "======PPO Validation from:  20191004 to  20200106\n",
            "PPO Sharpe Ratio:  -0.21163447052393433\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.5223415295283  minutes\n",
            "======TD3 Validation from:  20191004 to  20200106\n",
            "TD3 Sharpe Ratio:  -0.18929871052292013\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.6919084429740905  minutes\n",
            "======SAC Validation from:  20191004 to  20200106\n",
            "SAC Sharpe Ratio:  -0.32788191379402215\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9209745208422343  minutes\n",
            "======DDPG Validation from:  20191004 to  20200106\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.a2c.a2c.A2C object at 0x7f3be1dcd438>\n",
            "======Trading from:  20200106 to  20200406\n",
            "previous_total_asset:1441536.0083776521\n",
            "end_total_asset:1424537.182483556\n",
            "total_reward:-16998.825894096168\n",
            "total_cost:  921.582268067006\n",
            "total trades:  179\n",
            "Sharpe:  -0.44738217319212437\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358324\n",
            "======Model training from:  20090000 to  20200106\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 24       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 4.16     |\n",
            "| reference_Q_std         | 3.25     |\n",
            "| reference_action_mean   | 0.0487   |\n",
            "| reference_action_std    | 0.975    |\n",
            "| reference_actor_Q_mean  | 5.08     |\n",
            "| reference_actor_Q_std   | 3.23     |\n",
            "| rollout/Q_mean          | 2.73     |\n",
            "| rollout/actions_mean    | 0.0621   |\n",
            "| rollout/actions_std     | 0.823    |\n",
            "| rollout/episode_steps   | 2.71e+03 |\n",
            "| rollout/episodes        | 3        |\n",
            "| rollout/return          | 207      |\n",
            "| rollout/return_history  | 207      |\n",
            "| total/duration          | 54.5     |\n",
            "| total/episodes          | 3        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 183      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -5.01    |\n",
            "| train/loss_critic       | 2.12     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.118    |\n",
            "--------------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -3.93e-06 |\n",
            "| fps                | 281       |\n",
            "| nupdates           | 100       |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 500       |\n",
            "| value_loss         | 0.301     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0243   |\n",
            "| fps                | 299      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.5      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00669 |\n",
            "| fps                | 305      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 2.5      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.265   |\n",
            "| fps                | 309      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 15.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.144    |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 156      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 309       |\n",
            "| nupdates           | 600       |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 3000      |\n",
            "| value_loss         | 0.651     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0374   |\n",
            "| fps                | 311      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 19.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.103    |\n",
            "| fps                | 312      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 4.36     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0902   |\n",
            "| fps                | 313      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 6.5      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.215    |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 1.21     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.364   |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 9.9      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.156    |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 0.0487   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.393   |\n",
            "| fps                | 314      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 20.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0312   |\n",
            "| fps                | 315      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 101      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0486  |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 7.46     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 565      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0034   |\n",
            "| fps                | 316      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 4.45     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0429   |\n",
            "| fps                | 317      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 9.74     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0185  |\n",
            "| fps                | 318      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 5.9      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 5.93     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0722   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 8.58     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0971   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 102      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.193    |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 8.57     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 0.96     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 2.95     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0652   |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 7.42     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 16.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0407   |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 1.08     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0569  |\n",
            "| fps                | 319      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 20.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 0.0691   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.456   |\n",
            "| fps                | 320      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 9.29     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.027    |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 5        |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 25.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.181    |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 12.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 321      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 7.88     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.71e-05 |\n",
            "| fps                | 322       |\n",
            "| nupdates           | 3600      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 18000     |\n",
            "| value_loss         | 0.634     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0375   |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 0.672    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 6.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.95    |\n",
            "| fps                | 322      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 0.899    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 323       |\n",
            "| nupdates           | 4000      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 20000     |\n",
            "| value_loss         | 8.37      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 323       |\n",
            "| nupdates           | 4100      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 20500     |\n",
            "| value_loss         | 12.7      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0518  |\n",
            "| fps                | 323      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 6.26     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00574 |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 3.11     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0123   |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 20.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0673  |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 10.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 1.69     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.137   |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 2.4      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.548   |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 2.57     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0588   |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 18.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.7     |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 16.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.25    |\n",
            "| fps                | 324      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 0.285    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.29     |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 2.71     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.05     |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 10.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0482  |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 20.7     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 325       |\n",
            "| nupdates           | 5500      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 27500     |\n",
            "| value_loss         | 4.17      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.168   |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 2.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 5.02     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.57    |\n",
            "| fps                | 325      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 6.63     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0909   |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 1.96     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0201  |\n",
            "| fps                | 326      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 17.1     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.5438034693400065  minutes\n",
            "======A2C Validation from:  20200106 to  20200406\n",
            "A2C Sharpe Ratio:  -0.4345731114360969\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.261185204982757  minutes\n",
            "======PPO Validation from:  20200106 to  20200406\n",
            "PPO Sharpe Ratio:  -0.4522090960327457\n",
            "======TD3 Training========\n",
            "Training time (TD3):  2.4759629487991335  minutes\n",
            "======TD3 Validation from:  20200106 to  20200406\n",
            "TD3 Sharpe Ratio:  -0.37672259554077675\n",
            "======SAC Training========\n",
            "Training time (SAC):  2.6831216692924498  minutes\n",
            "======SAC Validation from:  20200106 to  20200406\n",
            "SAC Sharpe Ratio:  -0.4328935965502179\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9175780773162842  minutes\n",
            "======DDPG Validation from:  20200106 to  20200406\n",
            "**************************************************\n",
            "picked up technique <stable_baselines.td3.td3.TD3 object at 0x7f3beb2588d0>\n",
            "======Trading from:  20200406 to  20200707\n",
            "previous_total_asset:1424537.182483556\n",
            "end_total_asset:1428938.6434835559\n",
            "total_reward:4401.460999999894\n",
            "total_cost:  595.97\n",
            "total trades:  77\n",
            "Sharpe:  0.2624541581748274\n",
            "Ensemble Strategy took:  229.05557493368784  minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPW_XgBaoTGJ"
      },
      "source": [
        "##### The total reward and cost has improved compared to other Ensemble strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC2q2bymoZxC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}